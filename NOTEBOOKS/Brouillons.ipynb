{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Brouillons.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"WP8X5LcmexVy","colab_type":"text"},"source":["### Optimisation of Preparing the axes for visualisation"]},{"cell_type":"code","metadata":{"id":"b1iriAqBgpPU","colab_type":"code","colab":{}},"source":["# # TESTING FOR n_neighbors=15\n","\n","# ind_sample = df_cust_trans.sample(1000, random_state=14).index\n","\n","# fig = plt.figure(figsize=(18,3))\n","# n_neigh_list = [5, 10, 20, 50, 100, 200]\n","# for i, n in enumerate(n_neigh_list,1):\n","#     draw_umap(data=df,\n","#             #   ser_clust=ser_clust.loc[ind_sample],#\n","#               n_neighbors=n,\n","#               fig=fig,\n","#               layout=str(1)+str(len(n_neigh_list))+str(i),\n","#               title='n_neighbors = {}'.format(n),\n","#               s=1, alpha=0.6,\n","#               random_state=14)\n","# fig.suptitle(\"UMAP : Testing for best nb neighbors\",\n","#              fontsize=16, fontweight='bold')\n","# plt.tight_layout(rect=[0,0,1,0.9])\n","# plt.show()\n","\n","# # TESTING FOR min_dist=0.1\n","\n","# fig = plt.figure(figsize=(18,3))\n","# min_dist_list = [0.0, 0.1, 0.25, 0.5, 0.8, 0.99]\n","# for i, d in enumerate(min_dist_list,1):\n","#     draw_umap(data=df,\n","#             #   ser_clust=ser_clust.loc[ind_sample],#\n","#               n_neighbors=100, min_dist=d,\n","#               fig=fig,\n","#               layout=str(1)+str(len(min_dist_list))+str(i),\n","#               title='min_dist = {}'.format(d),\n","#               s=1, alpha=0.6,\n","#               random_state=14)\n","# fig.suptitle(\"UMAP : Testing for best min distance\",\n","#              fontsize=16, fontweight='bold')\n","# plt.tight_layout(rect=[0,0,1,0.9])\n","# plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_SulyBVVwxGx","colab_type":"code","colab":{}},"source":["# # TESTING FOR n_components\n","\n","# fig = plt.figure(figsize=(18,5))\n","# n_comp_list = [1,2,3]\n","# for i, a in enumerate(n_comp_list,1):\n","#     draw_umap(data=df_cust_trans.loc[ind_sample],\n","#             #   ser_clust=ser_clust.loc[ind_sample],#\n","#               n_neighbors=100, min_dist=0.8,\n","#               n_components=a,\n","#               fig=fig, layout=str(1)+str(len(n_comp_list))+str(i),\n","#               title='min_dist = {}'.format(a),\n","#               s=3, alpha=0.6,\n","#               random_state=14)\n","# fig.suptitle(\"UMAP : Testing for best min distance\",\n","#              fontsize=16, fontweight='bold')\n","# plt.tight_layout(rect=[0,0,1,0.9])\n","# plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gu8TGTbJUKta","colab_type":"text"},"source":["# OLD"]},{"cell_type":"code","metadata":{"id":"TNNMdG6bRNY_","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib import cm\n","from sklearn.cluster import KMeans\n","\n","def clusters_ratio(df, n_clust, figsize=(15, 3)):\n","    fig = plt.figure(figsize=figsize)\n","    for i, k in enumerate(n_clust, 1):\n","        # Computing Kmeans with k clusters on scaled data\n","        km = KMeans(n_clusters=k)\n","        km.fit(df)\n","        # Computes cluster number (keeping original indices)\n","        ser_clust = pd.Series(km.labels_, index=df.index)\n","\n","        # Compute pct of clients in each cluster\n","        pop_perc = 100 * ser_clust.value_counts() / df.shape[0]\n","        pop_perc.sort_index(inplace=True)\n","\n","        ax = fig.add_subplot(str(1) + str(len(n_clust)) + str(i))\n","        ax.pie(pop_perc, autopct='%1.0f%%', pctdistance=0.5)\n","        ax.set_title(f'{str(k)} clusters')  # , pad=20\n","    fig.suptitle('Clusters ratio', fontsize=16, fontweight='bold')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9BRrB-1RNdz","colab_type":"code","colab":{}},"source":["''' Computes Silhouette, Davies-Bouldin, Calinsky-Harabasz and inertia scores\n","for different number of clusters, then aggregate (mean std)\n","and plot the mean scores as a function of the number of clusters,\n","returns the aggregated scores as a dataframe'''\n","\n","from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n","\n","def plot_clust_scores_vs_n_clust(df, n_clust=range(2,8),\n","                                 n_iter=10, figsize=(15,3)):\n","\n","    silh_df, dav_bould_df, cal_harab_df, distor_df = \\\n","        pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n","\n","    # --- Looping on the number of clusters to compute the scores\n","    score_df_agg = pd.DataFrame()\n","    for i, n in enumerate(n_clust,1):\n","        silh, dav_bould, cal_harab, distor = [], [], [], []\n","        # Iterations of the same model (stability)\n","        for j in range(n_iter): \n","            km = KMeans(n_clusters=n, n_jobs=-1)\n","            km.fit(df)\n","            ser_clust = pd.Series(data=km.labels_, index=df.index)\n","            # Computing scores for iterations\n","            silh.append(silhouette_score(X=df,\n","                                        labels=ser_clust))\n","            dav_bould.append(davies_bouldin_score(X=df,\n","                                        labels=ser_clust))\n","            cal_harab.append(calinski_harabasz_score(X=df,\n","                                        labels=ser_clust))\n","            distor.append(km.inertia_)                        \n","        # Dataframe of the results on iterations\n","        score_df = pd.DataFrame({'Silhouette': silh,\n","                                 'Davies_Bouldin': dav_bould,\n","                                 'Calinsky_Harabasz': cal_harab,\n","                                 'Distortion': distor})\n","        # --- Aggregation of the results\n","        ser_scores = score_df.agg(['mean', 'median', 'std']).unstack()\\\n","                        .rename(n)\n","        score_df_agg = pd.concat([score_df_agg, ser_scores], axis=1)\n","\n","    def gen_li(name): return [(name, s) for s in ('mean', 'median', 'std')]\n","    silh_df = score_df_agg.loc[gen_li('Silhouette')]\n","    dav_bould_df = score_df_agg.loc[gen_li('Davies_Bouldin')]\n","    cal_harab_df = score_df_agg.loc[gen_li('Calinsky_Harabasz')]\n","    distor_df = score_df_agg.loc[gen_li('Distortion')]\n","\n","    # --- Plotting the results\n","\n","    fig = plt.figure(figsize=figsize)\n","\n","    def score_plot_vs_nb_clust(score_df, name, ax, c=None):\n","        score_df.T[(name,'mean')].plot(yerr=score_df.T[(name,'std')], elinewidth=1,\n","                            capsize=2, capthick=1, ecolor='k', fmt='-o',\n","                            c=c, ms=5, barsabove=False, uplims=False, ax=ax)\n","        \n","    ax = fig.add_subplot(141)\n","    score_plot_vs_nb_clust(silh_df,'Silhouette', ax, c='r')\n","    ax.set_xlabel('Number of clusters')\n","    ax.set_ylabel('Silhouette score')\n","\n","    ax = fig.add_subplot(142)\n","    score_plot_vs_nb_clust(dav_bould_df,'Davies_Bouldin', ax, c='b')\n","    ax.set_xlabel('Number of clusters')\n","    ax.set_ylabel('Davies_Bouldin score')\n","\n","    ax = fig.add_subplot(143)\n","    score_plot_vs_nb_clust(cal_harab_df,'Calinsky_Harabasz', ax, c='purple')\n","    ax.set_xlabel('Number of clusters')\n","    ax.set_ylabel('Calinsky_Harabasz score')\n","\n","    ax = fig.add_subplot(144)\n","    score_plot_vs_nb_clust(distor_df,'Distortion', ax, c='g')\n","    ax.set_xlabel('Number of clusters')\n","    ax.set_ylabel('Distortion')\n","\n","    fig.suptitle('Clustering score vs. number of clusters',\n","                fontsize=14, fontweight='bold')\n","    plt.tight_layout(rect=[0,0,1,0.95])\n","    plt.show()\n","\n","    return score_df_agg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"btckeG4tp7Gq","colab_type":"code","colab":{}},"source":["'''Computes the relative difference between the mean and the mean\n","of each clusters for each features of the original dataframe \n","'''\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib import cm\n","from sklearn.cluster import KMeans\n","\n","def mean_dev_clust(model, df, orig_df, palette='seismic', figsize=(20, 3)):\n","    # Assign segment to each customer in original dataset\n","    data_with_clust = orig_df.assign(cluster=model.labels_)\n","    k = data_with_clust['cluster'].nunique()\n","\n","    # Compute average for each feature by cluster\n","    kmeans_averages = data_with_clust.groupby(['cluster']).mean().round(2)\n","    print('Mean val for each cluster: ')\n","    display(kmeans_averages)\n","    print(\"\\n\")\n","\n","    # Ratio of difference between mean and cluster means for each feature\n","    rel_variation = 100 * (kmeans_averages - orig_df.mean()) \\\n","                    / (orig_df.mean() + 0.1)\n","\n","    # Plotting figure\n","    fig = plt.figure(figsize=figsize)\n","    ax1 = fig.add_subplot(111)\n","    vlim = np.array([abs(rel_var.min().min()),\n","                     abs(rel_var.max().max())]).max()\n","    sns.heatmap(data=rel_variation,\n","                # vmin=-vlim, vmax=vlim,\n","                center=0, annot=True, fmt='.0f',\n","                cmap=palette, ax=ax1)\n","    ax1.set_title('Mean deviation to the mean (%)', pad=20)\n","    ax1.set_ylabel(ylabel='cluster', labelpad=20)\n","    return rel_variation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OoBwUiqDZ5At","colab_type":"code","colab":{}},"source":["from sklearn.metrics import adjusted_rand_score\n","\n","def stability(model, df, n_iter=5, n_samp=10000):\n","\n","    multiple_ser_clust = []\n","    for i in range(n_iter):\n","        model.fit(df)\n","        multiple_ser_clust.append(model.labels_)\n","    print(\"--- Testing for initialisation stability \\\n","({} iterations) ---\".format(n_iter))\n","\n","    # Computes ARI scores for each pair of models\n","    ARI_scores = []\n","    pairs_list = combinlist(np.arange(n_iter),2)\n","    for i, j in pairs_list:\n","        ARI_scores.append(adjusted_rand_score(multiple_ser_clust[i],\n","                                              multiple_ser_clust[j]))\n","\n","    # Compute the mean and standard deviation of ARI scores\n","    ARI_mean, ARI_std = np.mean(ARI_scores), np.std(ARI_scores)\n","    print(\"Evaluation of stability with random init :\\n\\\n","        mean:{:.1f} , std: {:.1f} \".format(ARI_mean, ARI_std))\n","\n","    return ARI_scores\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-AUjRrv3UIQC","colab_type":"text"},"source":["# NEW"]},{"cell_type":"code","metadata":{"id":"b1sDR8xxToKq","colab_type":"code","colab":{}},"source":["''' For a each number of clusters in a list (list_n_clust),\n","- runs iterations (n_iter times) of a KMeans on a given dataframe,\n","- computes the 4 scores : silhouette, davies-bouldin, calinski_harabasz and\n","distortion\n","- if enabled only(return_pop): the proportion (pct) of the clusters\n","for each iteration and number of clusters\n","- and returns 3 dictionnaries:\n","    - dict_scores_iter: the 4 scores\n","    - dict_ser_clust_n_clust: the list of clusters labels for df rows\n","    - if enabled only (return_pop), dict_pop_perc_n_clust : the proportions\n","\n","NB: the functions plot_scores_vs_n_clust and plot_prop_clust_vs_nclust plot\n","respectively the scores vs the number of clusters and the proportion of\n","clusters based on the dictionnaries provided by compute_clust_scores_nclust'''\n","\n","def compute_clust_scores_nclust(df, list_n_clust=range(2,8),\n","                                 n_iter=10, return_pop=False):\n","\n","    dict_pop_perc_n_clust = {}\n","    dict_ser_clust_n_clust = {}\n","    dict_scores_iter = {}\n","\n","    # --- Looping on the number of clusters to compute the scores\n","    for i, n_clust in enumerate(list_n_clust,1):\n","\n","        silh, dav_bould, cal_harab, distor = [], [], [], []\n","        pop_perc_iter, ser_clust_iter = pd.DataFrame(), pd.DataFrame()\n","\n","        # Iterations of the same model (stability)\n","        for j in range(n_iter): \n","            km = KMeans(n_clusters=n_clust, n_jobs=-1)\n","            km.fit(df)\n","            ser_clust = pd.Series(data=km.labels_,\n","                                  index=df.index, \n","                                  name=\"iter_\"+str(j))\n","            ser_clust_iter = pd.concat([ser_clust_iter, ser_clust.to_frame()],\n","                                       axis=1)\n","\n","            if return_pop:\n","                # Compute pct of clients in each cluster\n","                pop_perc = 100 * ser_clust.value_counts() / df.shape[0]\n","                pop_perc.sort_index(inplace=True)\n","                pop_perc.index = ['clust_'+str(i) for i in pop_perc.index]\n","                pop_perc_iter = pd.concat([pop_perc_iter, pop_perc.to_frame()],\n","                                          axis=1)\n","        \n","            # Computing scores for iterations\n","            silh.append(silhouette_score(X=df, labels=ser_clust))\n","            dav_bould.append(davies_bouldin_score(X=df, labels=ser_clust))\n","            cal_harab.append(calinski_harabasz_score(X=df, labels=ser_clust))\n","            distor.append(km.inertia_)\n","\n","        dict_ser_clust_n_clust[n_clust] = ser_clust_iter\n","\n","        if return_pop:\n","            # dict of the population (pct) of clusters iterations\n","             dict_pop_perc_n_clust[n_clust] = pop_perc_iter.T\n","\n","        # Dataframe of the results on iterations\n","        scores_iter = pd.DataFrame({'Silhouette': silh,\n","                                 'Davies_Bouldin': dav_bould,\n","                                 'Calinsky_Harabasz': cal_harab,\n","                                 'Distortion': distor})\n","        dict_scores_iter[n_clust] = scores_iter\n","\n","    if return_pop:\n","        return dict_scores_iter, dict_ser_clust_n_clust, dict_pop_perc_n_clust\n","    else:\n","        return dict_scores_iter, dict_ser_clust_n_clust"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V69siMjPToHL","colab_type":"code","colab":{}},"source":["''' Plot the 4 mean scores stored in the dictionnary returned by the function\n","compute_clust_scores_nclust (dictionnary of dataframes of scores (columns)\n","for each iteration (rows) of the model and for each number of clusters\n","in a figure with error bars (2 sigmas)'''\n","\n","def plot_scores_vs_n_clust(dict_scores_iter, figsize=(15,3)):\n","\n","    fig = plt.figure(figsize=figsize)\n","    list_n_clust = list(dict_scores_iter.keys())\n","\n","    # Generic fonction to unpack dictionary and plot one graph\n","    def score_plot_vs_nb_clust(scores_iter, name, ax, c=None):\n","        score_mean = [dict_scores_iter[i].mean().loc[n_score] for i in list_n_clust]\n","        score_std = np.array([dict_scores_iter[i].std().loc[n_score]\\\n","                            for i in list_n_clust])\n","        ax.errorbar(list_n_clust, score_mean, yerr=2*score_std, elinewidth=1,\n","                capsize=2, capthick=1, ecolor='k', fmt='-o', c=c, ms=5,\n","                barsabove=False, uplims=False)\n","\n","    li_scores = ['Silhouette', 'Davies_Bouldin',\n","                 'Calinsky_Harabasz', 'Distortion']\n","    li_colors = ['r', 'b', 'purple', 'g']\n","\n","    # Looping on the 4 scores\n","    i=0\n","    for n_score, c in zip(li_scores, li_colors):\n","        i+=1\n","        ax = fig.add_subplot(1,4,i)\n","        \n","        score_plot_vs_nb_clust(dict_scores_iter, n_score, ax, c=c)\n","        ax.set_xlabel('Number of clusters')\n","        ax.set_ylabel(n_score+' score')\n","\n","    fig.suptitle('Clustering score vs. number of clusters',\n","                fontsize=14, fontweight='bold')\n","    plt.tight_layout(rect=[0,0,1,0.95])\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cdHdS8PnTuzv","colab_type":"code","colab":{}},"source":["''' Plot the proportion (%) of each cluster (columns) returned by the function\n","compute_clust_scores_nclust (dictionnary of dataframes of the proportion\n","for each iteration (rows) of the model in one figure with error bars (2 sigmas)'''\n","\n","def plot_prop_clust_vs_nclust(dict_pop_perc_n_clust, figsize=(15,3)):\n","\n","    fig = plt.figure(figsize=figsize)\n","    list_n_clust = list(dict_scores_iter.keys())\n","\n","    for i, n_clust in enumerate(list_n_clust, 1):\n","        ax = fig.add_subplot(3,3,i)\n","        sns.stripplot(data=dict_pop_perc_n_clust[n_clust],\n","                      edgecolor='k', linewidth=1,  ax=ax)\n","        ax.set(ylim=(0,100))\n","        ax.set_ylabel(\"prop. of the clusters (%)\")\n","    fig.suptitle(f\"Proportion of the clusters through {n_iter} iterations\",\n","                fontweight='bold', fontsize=14)\n","    plt.tight_layout(rect=[0,0,1,0.97])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E5Zf0R8TixeA","colab_type":"code","colab":{}},"source":["\"\"\" Plot pies of the proportion of the clusters using the proportions\n","stored in the dictionnary returned by the function\n","'compute_clust_scores_nclust' (dictionnary of dataframes of the\n","proportions (columns) for each iteration (rows) of the model\n","and for each number of clusters in a figure with error (+/-2 sigmas)\"\"\"\n","\n","def plot_clust_prop_pie_vs_nclust(dict_pop_perc_n_clust,\n","                                  list_n_clust, figsize=(15, 3)):\n","\n","    fig = plt.figure(figsize=figsize)\n","\n","    for i, n_clust in enumerate(list_n_clust,1):\n","        ax = fig.add_subplot(str(1) + str(len(list_n_clust)) + str(i))\n","\n","        mean_ = dict_pop_perc_n_clust[n_clust].mean()\n","        std_ = dict_pop_perc_n_clust[n_clust].std()\n","        \n","        wedges, texts, autotexts = ax.pie(mean_,\n","                autopct='%1.0f%%',\n","                labels=[\"(+/-{:.0f})\".format(i) for i in std_.values],\n","                pctdistance=0.5)\n","        plt.setp(autotexts, size=10, weight=\"bold\")\n","        plt.setp(texts, size=8)\n","        ax.set_title(f'{str(n_clust)} clusters')  # , pad=20\n","\n","    fig.suptitle('Clusters ratio', fontsize=16, fontweight='bold')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jsTjaCaPqAbG","colab_type":"code","colab":{}},"source":["''' For each quantitative value of the original dataframe\n","(prior to transformation and clustering), returns two dataframes:\n","- the mean value for each clusters\n","- the relative difference of the means between clusters and whole dataframe '''\n","\n","def mean_deviation_clust(model, df, orig_df, palette='seismic', figsize=(20, 3)):\n","\n","    # Filters the numeric datas in 'orig_df'\n","    orig_df_quant = orig_df.select_dtypes(include=[np.number])\n","    model = model.fit(df) if not is_fitted(model) else model    \n","\n","    # Assign segment to each customer in original dataset\n","    data_with_clust = orig_df_quant.assign(cluster=model.labels_)\n","    k = data_with_clust['cluster'].nunique()\n","\n","    # Compute average for each feature by cluster\n","    clust_mean = data_with_clust.groupby(['cluster']).mean().round(2)\n","\n","    # Ratio of difference between mean and cluster means for each feature\n","    orig_df_mean = orig_df_quant.mean()\n","    rel_var = 100 * (clust_mean - orig_df_mean) \\\n","                    / (orig_df_mean + 0.1)\n","\n","    # Plotting figure\n","    fig = plt.figure(figsize=figsize)\n","    ax1 = fig.add_subplot(111)\n","    vlim = np.array([abs(rel_var.min().min()),\n","                     abs(rel_var.max().max())]).max()\n","    sns.heatmap(data=rel_var,\n","                vmin=-vlim, vmax=vlim,\n","                center=0, annot=True, fmt='.0f',\n","                cmap=palette, ax=ax1)\n","    ax1.set_title('Mean deviation to the mean (%)', pad=20)\n","    ax1.set_ylabel(ylabel='cluster', labelpad=20)\n","\n","    return clust_mean, orig_df_mean, rel_var"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0C5uJdHVYO58","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MXr7YhgqYO9c","colab_type":"code","colab":{}},"source":["# A REFONDRE EN UTILISANT LE DICTIONNAIRE dict_ser_clust_n_clust\n","\n","def ARI_all_pairs(df_ser_clust_iter, first_vs_others=False):\n","\n","    n_columns = len(df_ser_clust_iter.columns)\n","    n_clust = df_ser_clust_iter.stack().nunique()\n","    \n","    # Computes ARI scores for each pair of models\n","    ARI_scores = []\n","    if first_vs_others: # first columns versus the others\n","        pairs_list = [[0,i] for i in range(1,n_columns)]\n","        print(\"--- ARI between first and the {} others ---\".format(n_columns-1))\n","        name = f'ARI_{str(n_clust)}_clust_first_vs_others'\n","    else: # all pairs\n","        pairs_list = combinlist(np.arange(n_columns),2)\n","        print(\"--- ARI all {} unique pairs ---\".format(len(pairs_list)))\n","        name = f'ARI_{str(n_clust)}_clust_all_pairs'\n","\n","    for i, j in pairs_list:\n","        ARI_scores.append(adjusted_rand_score(df_ser_clust_iter.iloc[:,i],\n","                                              df_ser_clust_iter.iloc[:,j]))\n","\n","    # Compute the mean and standard deviation of ARI scores\n","    ARI_mean, ARI_std = np.mean(ARI_scores), np.std(ARI_scores)\n","    ARI_min, ARI_max = np.min(ARI_scores), np.max(ARI_scores)\n","    print(\"Evaluation of stability with random init :\\n\\\n","            ARI: mean={:.3f}, std={:.3f}, min={:.3f}, max={:.3f} \"\\\n","            .format(ARI_mean, ARI_std, ARI_min, ARI_max))\n","\n","    return pd.Series(ARI_scores, index=pd.Index(pairs_list),\n","                     name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y3gRKW0cYPBA","colab_type":"code","colab":{}},"source":["''' Plots a radar chart of the cluster profiles from a dataframe containing:\n","- the name or number of cluster as index\n","- the means of each features per clusters in columns\n","OR\n","- the value of each features for each cluster center (centroid)\n","NB: the values are almost the same\n","\n","Values are scaled using a MinMaxScaler'''\n","\n","from sklearn.preprocessing import MinMaxScaler\n"," \n","def plot_radar_chart(df, row, title, color, min_max_scaling=False, ax=None):\n","\n","    df_copy = df.copy('deep')\n","    if min_max_scaling:\n","        min_max = MinMaxScaler()\n","        df_copy = pd.DataFrame(min_max.fit_transform(df_copy),\n","                               index=df_copy.index,\n","                               columns=df_copy.columns)\n","    df_ = df_copy.reset_index()\n","    categories=list(df)\n","    n_vars = len(categories)\n","    \n","    angles = [n / float(N) * 2 * pi for n in range(n_vars)]\n","    angles += angles[:1] # \"complete the loop\"\n","    \n","    ax = plt.subplot(1,1,1, polar=True) if ax is None else ax\n","    \n","    # First axis to be on top:\n","    ax.set_theta_offset(pi / 2)\n","    ax.set_theta_direction(-1)\n","    \n","    plt.xticks(angles[:-1], categories, color='grey', size=8)\n","    \n","    ax.set_rlabel_position(0)\n","    plt.yticks(color=\"grey\", size=7)# [10,20,30], [\"10\",\"20\",\"30\"], \n","    # plt.ylim(0,40)\n","    \n","    values=df_.loc[row].drop('clust').values.flatten().tolist() # \n","    values += values[:1]\n","    ax.plot(angles, values, color=color, linewidth=2, linestyle='solid')\n","    ax.fill(angles, values, color=color, alpha=0.4)\n","    \n","    plt.title(title, size=11, color=color, y=1.1,\n","              fontweight='bold')"],"execution_count":null,"outputs":[]}]}